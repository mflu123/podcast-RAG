{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ade514f6",
   "metadata": {},
   "source": [
    "THIS FILE DOES NOT NEED TO BE RUN BY GRADERS, AND IS ONLY FOR THE VIDEO WALKTHROUGH. \n",
    "This notebook was used to combine the original datasets into transcripts_full.json, which is the data that is used for the database. That data can be accessed here: \n",
    "\n",
    "https://duke.box.com/s/to632zn4o0pdfezhubtj0kkoc5rfi8kj\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ddad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74bec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranscriptDataManager:\n",
    "    \"\"\"\n",
    "    A class to manage loading, merging, and accessing podcast transcript data.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_path: str = \"..\"):\n",
    "        \"\"\"\n",
    "        Initialize the manager.\n",
    "        \n",
    "        Args:\n",
    "            base_path (str): The directory where the JSON files are located relative to the notebook.\n",
    "        \"\"\"\n",
    "        self.base_path = Path(base_path)\n",
    "        self.data: Dict[str, List[Dict[str, Any]]] = {}\n",
    "\n",
    "    def load_data(self, filenames: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Loads multiple JSON transcript files and merges them into a single data source.\n",
    "        \n",
    "        Args:\n",
    "            filenames (List[str]): List of filenames to load.\n",
    "        \"\"\"\n",
    "        for filename in filenames:\n",
    "            file_path = self.base_path / filename\n",
    "            if not file_path.exists():\n",
    "                print(f\"Warning: File not found: {file_path.resolve()}\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                print(f\"Loading {filename}...\")\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    chunk = json.load(f)\n",
    "                    self._merge_chunk(chunk)\n",
    "                print(f\"Successfully loaded {filename}. Total episodes so far: {len(self.data)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filename}: {e}\")\n",
    "\n",
    "    def _merge_chunk(self, new_data: Dict[str, List[Dict[str, Any]]]) -> None:\n",
    "        \"\"\"\n",
    "        Merges a new chunk of data into the main dataset.\n",
    "        \n",
    "        Args:\n",
    "            new_data (Dict): Dictionary of episodes to merge.\n",
    "        \"\"\"\n",
    "        existing_keys = set(self.data.keys())\n",
    "        new_keys = set(new_data.keys())\n",
    "        overlap = existing_keys.intersection(new_keys)\n",
    "        \n",
    "        if overlap:\n",
    "            print(f\"Warning: {len(overlap)} episodes overlap and will be overwritten.\")\n",
    "            \n",
    "        self.data.update(new_data)\n",
    "\n",
    "    def get_combined_data(self) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        \"\"\"Returns the raw combined dictionary.\"\"\"\n",
    "        return self.data\n",
    "\n",
    "    def get_all_utterances(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Flattens the data structure to return a list of all utterances across all episodes.\n",
    "        Useful for creating RAG chunks.\n",
    "        \"\"\"\n",
    "        all_utterances = []\n",
    "        for episode_id, utterances in self.data.items():\n",
    "            for u in utterances:\n",
    "                if 'episode_id' not in u:\n",
    "                    u['episode_id'] = episode_id\n",
    "            all_utterances.extend(utterances)\n",
    "        return all_utterances\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, int]:\n",
    "        \"\"\"Returns basic statistics about the loaded data.\"\"\"\n",
    "        return {\n",
    "            \"total_episodes\": len(self.data),\n",
    "            \"total_utterances\": sum(len(u) for u in self.data.values())\n",
    "        }\n",
    "\n",
    "    def save_to_json(self, output_filename: str) -> None:\n",
    "        \"\"\"\n",
    "        Saves the combined data to a JSON file.\n",
    "        \n",
    "        Args:\n",
    "            output_filename (str): The name of the file to save.\n",
    "        \"\"\"\n",
    "        output_path = self.base_path / output_filename\n",
    "        try:\n",
    "            print(f\"Saving data to {output_path}...\")\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.data, f, indent=4)\n",
    "            print(f\"Successfully saved data to {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43db666b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train-transcripts-aligned.json...\n",
      "Successfully loaded train-transcripts-aligned.json. Total episodes so far: 593\n",
      "Loading valid-transcripts-aligned.json...\n",
      "Successfully loaded valid-transcripts-aligned.json. Total episodes so far: 627\n",
      "Loading test-transcripts-aligned.json...\n",
      "Successfully loaded test-transcripts-aligned.json. Total episodes so far: 663\n",
      "\n",
      "Data Loading Complete.\n",
      "Total Episodes: 663\n",
      "Total Utterances: 163808\n",
      "Successfully loaded train-transcripts-aligned.json. Total episodes so far: 593\n",
      "Loading valid-transcripts-aligned.json...\n",
      "Successfully loaded valid-transcripts-aligned.json. Total episodes so far: 627\n",
      "Loading test-transcripts-aligned.json...\n",
      "Successfully loaded test-transcripts-aligned.json. Total episodes so far: 663\n",
      "\n",
      "Data Loading Complete.\n",
      "Total Episodes: 663\n",
      "Total Utterances: 163808\n"
     ]
    }
   ],
   "source": [
    "files_to_load = [\n",
    "    \"train-transcripts-aligned.json\",\n",
    "    \"valid-transcripts-aligned.json\",\n",
    "    \"test-transcripts-aligned.json\"\n",
    "]\n",
    "\n",
    "manager = TranscriptDataManager(base_path=\"..\")\n",
    "\n",
    "manager.load_data(files_to_load)\n",
    "\n",
    "stats = manager.get_stats()\n",
    "print(\"\\nData Loading Complete.\")\n",
    "print(f\"Total Episodes: {stats['total_episodes']}\")\n",
    "print(f\"Total Utterances: {stats['total_utterances']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ada042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d23e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to ../combined_transcripts.json...\n",
      "Successfully saved data to ../combined_transcripts.json\n",
      "Successfully saved data to ../combined_transcripts.json\n"
     ]
    }
   ],
   "source": [
    "output_filename = \"combined_transcripts.json\"\n",
    "manager.save_to_json(output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e87c05a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
