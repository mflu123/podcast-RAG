{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b06bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain openai chromadb tiktoken jq python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d862f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain_core.documents import Document\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "import time\n",
    "import chromadb\n",
    "import gc\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4faa1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_episode_act(docs: List[Document]) -> Dict[Tuple[str, str], List[Document]]:\n",
    "    \"\"\"\n",
    "    Group documents by (episode, act).\n",
    "    \"\"\"\n",
    "    grouped: Dict[Tuple[str, str], List[Document]] = defaultdict(list)\n",
    "    for doc in docs:\n",
    "        episode = doc.metadata.get(\"episode\", \"unknown_episode\")\n",
    "        act = doc.metadata.get(\"act\", \"unknown_act\")\n",
    "        grouped[(episode, act)].append(doc)\n",
    "    return grouped\n",
    "\n",
    "\n",
    "def sort_group_by_utterance_start(group: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Sort a list of docs by utterance_start (ascending).\n",
    "    Missing values are treated as 0.\n",
    "    \"\"\"\n",
    "    return sorted(\n",
    "        group,\n",
    "        key=lambda d: (d.metadata.get(\"utterance_start\")\n",
    "                       if d.metadata.get(\"utterance_start\") is not None\n",
    "                       else 0.0)\n",
    "    )\n",
    "\n",
    "\n",
    "class TranscriptChunker:\n",
    "    \"\"\"\n",
    "    Chunk This American Lifeâ€“style transcripts into smaller text blocks,\n",
    "    grouped by episode + act and ordered by utterance_start.\n",
    "\n",
    "    Chunks are formed by concatenating utterances until `max_words`\n",
    "    is reached, with optional overlap in terms of utterances.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 max_words: int = 350,\n",
    "                 overlap_utterances: int = 2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_words: Target maximum words per chunk.\n",
    "            overlap_utterances: How many utterances from the end of the\n",
    "                                previous chunk to repeat at the start\n",
    "                                of the next chunk.\n",
    "        \"\"\"\n",
    "        self.max_words = max_words\n",
    "        self.overlap_utterances = overlap_utterances\n",
    "\n",
    "    def _make_chunk_document(\n",
    "        self,\n",
    "        episode: str,\n",
    "        act: str,\n",
    "        chunk_index: int,\n",
    "        docs_in_chunk: List[Document]\n",
    "    ) -> Document:\n",
    "        \"\"\"\n",
    "        Create a new Document representing one chunk, with aggregated metadata.\n",
    "        \"\"\"\n",
    "        text_parts = [d.page_content.strip() for d in docs_in_chunk if d.page_content]\n",
    "        chunk_text = \" \".join(text_parts)\n",
    "\n",
    "        # Aggregate metadata\n",
    "        speakers = {d.metadata.get(\"speaker\") for d in docs_in_chunk if d.metadata.get(\"speaker\")}\n",
    "        roles = {d.metadata.get(\"role\") for d in docs_in_chunk if d.metadata.get(\"role\")}\n",
    "\n",
    "        starts = [d.metadata.get(\"utterance_start\") for d in docs_in_chunk\n",
    "                  if d.metadata.get(\"utterance_start\") is not None]\n",
    "        ends = [d.metadata.get(\"utterance_end\") for d in docs_in_chunk\n",
    "                if d.metadata.get(\"utterance_end\") is not None]\n",
    "\n",
    "        chunk_metadata: Dict[str, Any] = {\n",
    "            \"episode\": episode,\n",
    "            \"act\": act,\n",
    "            \"chunk_index\": chunk_index,\n",
    "            \"num_utterances\": len(docs_in_chunk),\n",
    "            \"num_words\": len(chunk_text.split()),\n",
    "            \"speakers\": \", \".join(sorted(list(speakers))),\n",
    "            \"roles\": \", \".join(sorted(list(roles))),\n",
    "            \"chunk_utterance_start\": min(starts) if starts else None,\n",
    "            \"chunk_utterance_end\": max(ends) if ends else None,\n",
    "        }\n",
    "\n",
    "        return Document(page_content=chunk_text, metadata=chunk_metadata)\n",
    "\n",
    "    def chunk_group(self,\n",
    "                    episode: str,\n",
    "                    act: str,\n",
    "                    docs_in_group: List[Document]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Chunk all utterances for a single (episode, act) group.\n",
    "        Assumes docs_in_group are already sorted by utterance_start.\n",
    "        \"\"\"\n",
    "        chunks: List[Document] = []\n",
    "        current_docs: List[Document] = []\n",
    "        current_word_count = 0\n",
    "        chunk_index = 0\n",
    "\n",
    "        for doc in docs_in_group:\n",
    "            words = doc.page_content.split()\n",
    "            n_words = len(words)\n",
    "\n",
    "            # If adding this utterance would exceed the max_words, flush current chunk\n",
    "            if current_docs and (current_word_count + n_words > self.max_words):\n",
    "                chunk_doc = self._make_chunk_document(\n",
    "                    episode, act, chunk_index, current_docs\n",
    "                )\n",
    "                chunks.append(chunk_doc)\n",
    "                chunk_index += 1\n",
    "\n",
    "                # prepare next chunk, with overlap\n",
    "                if self.overlap_utterances > 0:\n",
    "                    overlap_docs = current_docs[-self.overlap_utterances:]\n",
    "                else:\n",
    "                    overlap_docs = []\n",
    "\n",
    "                current_docs = list(overlap_docs)\n",
    "                current_word_count = sum(\n",
    "                    len(d.page_content.split()) for d in current_docs\n",
    "                )\n",
    "\n",
    "            # add current utterance\n",
    "            current_docs.append(doc)\n",
    "            current_word_count += n_words\n",
    "\n",
    "        # flush final chunk\n",
    "        if current_docs:\n",
    "            chunk_doc = self._make_chunk_document(\n",
    "                episode, act, chunk_index, current_docs\n",
    "            )\n",
    "            chunks.append(chunk_doc)\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def chunk_documents(self, docs: List[Document]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        High-level method: groups by (episode, act), sorts within each group,\n",
    "        and returns a flat list of chunk Documents.\n",
    "        \"\"\"\n",
    "        grouped = group_by_episode_act(docs)\n",
    "        all_chunks: List[Document] = []\n",
    "\n",
    "        for (episode, act), group_docs in grouped.items():\n",
    "            sorted_docs = sort_group_by_utterance_start(group_docs)\n",
    "            group_chunks = self.chunk_group(episode, act, sorted_docs)\n",
    "            all_chunks.extend(group_chunks)\n",
    "\n",
    "        return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a35c13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "    metadata[\"episode\"] = record.get(\"episode\")\n",
    "    metadata[\"role\"] = record.get(\"role\")\n",
    "    metadata[\"speaker\"] = record.get(\"speaker\")\n",
    "    metadata[\"act\"] = record.get(\"act\")\n",
    "    metadata[\"utterance_start\"] = record.get(\"utterance_start\")\n",
    "    metadata[\"utterance_end\"] = record.get(\"utterance_end\")\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aff5257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 163808 utterances.\n"
     ]
    }
   ],
   "source": [
    "loader = JSONLoader(\n",
    "    file_path=\"../data/transcripts_full.json\",\n",
    "    jq_schema=\".[].[]\",     \n",
    "    content_key=\"utterance\",  \n",
    "    metadata_func=metadata_func\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "print(f\"Loaded {len(docs)} utterances.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17429d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original utterances: 163808\n",
      "Chunked docs: 28642\n",
      "{'episode': 'ep-1', 'act': 'prologue', 'chunk_index': 0, 'num_utterances': 15, 'num_words': 342, 'speakers': 'ira glass, joe franklin', 'roles': 'host, interviewer, subject', 'chunk_utterance_start': 0.17, 'chunk_utterance_end': 132.75}\n",
      "Joe Franklin? I'm ready. It's Ira Glass here. Oh, you're the emcee on the show, Ira. I am the emcee on the show. Yes. Oh great. Ira? I-R-A, Ira? Ira, I-R-A. Oh, great. Now hold on one second, Ira. Don't go away. Hello? [UNINTELLIGIBLE]. Call me after 3 o'clock. I have great news for you. Ira. Yes. So listen, Tony. If the phone rings, take it in the back. And then come out and tell me who it is. Ju ...\n"
     ]
    }
   ],
   "source": [
    "# Combine docs into larger chunks\n",
    "chunker = TranscriptChunker(\n",
    "    max_words=350,          # you can tune this\n",
    "    overlap_utterances=2    # you can tune this too\n",
    ")\n",
    "\n",
    "chunked_docs = chunker.chunk_documents(docs)\n",
    "\n",
    "print(f\"Original utterances: {len(docs)}\")\n",
    "print(f\"Chunked docs: {len(chunked_docs)}\")\n",
    "if chunked_docs:\n",
    "    print(chunked_docs[0].metadata)\n",
    "    print(chunked_docs[0].page_content[:400], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720e7250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens to embed: 10993831\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "\n",
    "total_tokens = 0\n",
    "for doc in chunked_docs:\n",
    "    total_tokens += len(tokenizer.encode(doc.page_content))\n",
    "\n",
    "print(f\"Total tokens to embed: {total_tokens}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed7602a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nt/6nrb_d696ylb20nhdcw60rfc0000gn/T/ipykernel_7415/3641289048.py:2: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting embedding generation for 28642 chunks...\n",
      "Embedded 28642 / 28642\n",
      "Finished embedding all chunks.\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# 2. Extract texts + metadata\n",
    "texts = [doc.page_content for doc in chunked_docs]\n",
    "metadatas = [doc.metadata for doc in chunked_docs]\n",
    "ids = [str(i) for i in range(len(texts))]\n",
    "\n",
    "# 3. Manual embedding with rate-limit protection\n",
    "batch_size = 20     # safe\n",
    "sleep_time = 0.15   # ~150 ms between batches\n",
    "\n",
    "vectors = []\n",
    "\n",
    "print(f\"Starting embedding generation for {len(texts)} chunks...\")\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch = texts[i:i+batch_size]\n",
    "    vec = embeddings.embed_documents(batch)\n",
    "    vectors.extend(vec)\n",
    "\n",
    "    print(f\"Embedded {i + len(batch)} / {len(texts)}\", end=\"\\r\")\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "print(\"\\nFinished embedding all chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e182ea6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ChromaDB persistence directory: /Users/matthewlu/Downloads/podcast-RAG/notebooks/../models/tal_chroma\n",
      "Upserting vectors to ChromaDB...\n",
      "Upserted 30000 / 28642\n",
      "Done writing to Chroma.\n",
      "Chroma vector store created and verified successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nt/6nrb_d696ylb20nhdcw60rfc0000gn/T/ipykernel_7415/2985609888.py:38: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_store = Chroma(\n"
     ]
    }
   ],
   "source": [
    "# 4. Build and Persist the Chroma DB\n",
    "\n",
    "# Force garbage collection to release any dangling file locks\n",
    "gc.collect()\n",
    "\n",
    "persist_dir = os.path.join(os.getcwd(), \"../models/tal_chroma\")\n",
    "print(f\"Using ChromaDB persistence directory: {persist_dir}\")\n",
    "\n",
    "try:\n",
    "    client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = client.get_or_create_collection(\"tal_collection\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing ChromaDB: {e}\")\n",
    "    print(\"If you see 'attempt to write a readonly database', please RESTART THE KERNEL.\")\n",
    "    raise e\n",
    "\n",
    "max_batch_size = 5000  # below 5461 limit\n",
    "\n",
    "print(\"Upserting vectors to ChromaDB...\")\n",
    "for start in range(0, len(ids), max_batch_size):\n",
    "    end = start + max_batch_size\n",
    "    batch_ids = ids[start:end]\n",
    "    batch_vectors = vectors[start:end]\n",
    "    batch_metas = metadatas[start:end]\n",
    "    batch_docs = texts[start:end]\n",
    "    \n",
    "    collection.upsert(\n",
    "        ids=batch_ids,\n",
    "        embeddings=batch_vectors,\n",
    "        metadatas=batch_metas,\n",
    "        documents=batch_docs,\n",
    "    )\n",
    "    print(f\"Upserted {end} / {len(ids)}\", end=\"\\r\")\n",
    "\n",
    "print(\"\\nDone writing to Chroma.\")\n",
    "\n",
    "# Verify creation\n",
    "vector_store = Chroma(\n",
    "    client=client,\n",
    "    collection_name=\"tal_collection\",\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "\n",
    "print(\"Chroma vector store created and verified successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d49a91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
