{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4985775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/anaconda3/lib/python3.12/site-packages (0.3.9)\n",
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.12/site-packages (1.54.4)\n",
      "Requirement already satisfied: chromadb in /opt/anaconda3/lib/python3.12/site-packages (1.3.5)\n",
      "Requirement already satisfied: tiktoken in /opt/anaconda3/lib/python3.12/site-packages (0.8.0)\n",
      "Requirement already satisfied: jq in /opt/anaconda3/lib/python3.12/site-packages (1.10.0)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.12/site-packages (0.21.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (0.3.21)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (0.3.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (0.1.143)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.32.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (9.1.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.7.1)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: build>=1.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (1.3.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (1.23.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (1.39.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (1.39.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (1.39.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (0.22.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (1.76.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (5.0.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (0.12.5)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (34.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (3.10.11)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (13.3.5)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from chromadb) (4.19.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: packaging>=19.1 in /opt/anaconda3/lib/python3.12/site-packages (from build>=1.0.3->chromadb) (23.2)\n",
      "Requirement already satisfied: pyproject_hooks in /opt/anaconda3/lib/python3.12/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (0.10.6)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.43.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.2.2)\n",
      "Requirement already satisfied: durationpy>=0.7 in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.21->langchain) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: coloredlogs in /opt/anaconda3/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/anaconda3/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
      "Requirement already satisfied: protobuf in /opt/anaconda3/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (6.33.1)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.0.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.39.0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.39.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.39.0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.39.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.60b0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (2.15.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /opt/anaconda3/lib/python3.12/site-packages (from tokenizers>=0.13.2->chromadb) (0.35.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.43.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.2.2)\n",
      "Requirement already satisfied: durationpy>=0.7 in /opt/anaconda3/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.21->langchain) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: coloredlogs in /opt/anaconda3/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/anaconda3/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
      "Requirement already satisfied: protobuf in /opt/anaconda3/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (6.33.1)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.0.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.39.0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.39.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.39.0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.39.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.60b0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (2.15.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /opt/anaconda3/lib/python3.12/site-packages (from tokenizers>=0.13.2->chromadb) (0.35.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.3.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.10)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain) (2.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->chromadb) (0.1.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.3.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.10)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain) (2.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->chromadb) (0.1.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/anaconda3/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/anaconda3/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain openai chromadb tiktoken jq python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "60e86b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain_core.documents import Document\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "import time\n",
    "import chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4189d155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_episode_act(docs: List[Document]) -> Dict[Tuple[str, str], List[Document]]:\n",
    "    \"\"\"\n",
    "    Group documents by (episode, act).\n",
    "    \"\"\"\n",
    "    grouped: Dict[Tuple[str, str], List[Document]] = defaultdict(list)\n",
    "    for doc in docs:\n",
    "        episode = doc.metadata.get(\"episode\", \"unknown_episode\")\n",
    "        act = doc.metadata.get(\"act\", \"unknown_act\")\n",
    "        grouped[(episode, act)].append(doc)\n",
    "    return grouped\n",
    "\n",
    "\n",
    "def sort_group_by_utterance_start(group: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Sort a list of docs by utterance_start (ascending).\n",
    "    Missing values are treated as 0.\n",
    "    \"\"\"\n",
    "    return sorted(\n",
    "        group,\n",
    "        key=lambda d: (d.metadata.get(\"utterance_start\")\n",
    "                       if d.metadata.get(\"utterance_start\") is not None\n",
    "                       else 0.0)\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c371649",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranscriptChunker:\n",
    "    \"\"\"\n",
    "    Chunk This American Lifeâ€“style transcripts into smaller text blocks,\n",
    "    grouped by episode + act and ordered by utterance_start.\n",
    "\n",
    "    Chunks are formed by concatenating utterances until `max_words`\n",
    "    is reached, with optional overlap in terms of utterances.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 max_words: int = 350,\n",
    "                 overlap_utterances: int = 2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_words: Target maximum words per chunk.\n",
    "            overlap_utterances: How many utterances from the end of the\n",
    "                                previous chunk to repeat at the start\n",
    "                                of the next chunk.\n",
    "        \"\"\"\n",
    "        self.max_words = max_words\n",
    "        self.overlap_utterances = overlap_utterances\n",
    "\n",
    "    def _make_chunk_document(\n",
    "        self,\n",
    "        episode: str,\n",
    "        act: str,\n",
    "        chunk_index: int,\n",
    "        docs_in_chunk: List[Document]\n",
    "    ) -> Document:\n",
    "        \"\"\"\n",
    "        Create a new Document representing one chunk, with aggregated metadata.\n",
    "        \"\"\"\n",
    "        text_parts = [d.page_content.strip() for d in docs_in_chunk if d.page_content]\n",
    "        chunk_text = \" \".join(text_parts)\n",
    "\n",
    "        # Aggregate metadata\n",
    "        speakers = {d.metadata.get(\"speaker\") for d in docs_in_chunk if d.metadata.get(\"speaker\")}\n",
    "        roles = {d.metadata.get(\"role\") for d in docs_in_chunk if d.metadata.get(\"role\")}\n",
    "\n",
    "        starts = [d.metadata.get(\"utterance_start\") for d in docs_in_chunk\n",
    "                  if d.metadata.get(\"utterance_start\") is not None]\n",
    "        ends = [d.metadata.get(\"utterance_end\") for d in docs_in_chunk\n",
    "                if d.metadata.get(\"utterance_end\") is not None]\n",
    "\n",
    "        chunk_metadata: Dict[str, Any] = {\n",
    "            \"episode\": episode,\n",
    "            \"act\": act,\n",
    "            \"chunk_index\": chunk_index,\n",
    "            \"num_utterances\": len(docs_in_chunk),\n",
    "            \"num_words\": len(chunk_text.split()),\n",
    "            \"speakers\": \", \".join(sorted(list(speakers))),\n",
    "            \"roles\": \", \".join(sorted(list(roles))),\n",
    "            \"chunk_utterance_start\": min(starts) if starts else None,\n",
    "            \"chunk_utterance_end\": max(ends) if ends else None,\n",
    "        }\n",
    "\n",
    "        return Document(page_content=chunk_text, metadata=chunk_metadata)\n",
    "\n",
    "    def chunk_group(self,\n",
    "                    episode: str,\n",
    "                    act: str,\n",
    "                    docs_in_group: List[Document]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Chunk all utterances for a single (episode, act) group.\n",
    "        Assumes docs_in_group are already sorted by utterance_start.\n",
    "        \"\"\"\n",
    "        chunks: List[Document] = []\n",
    "        current_docs: List[Document] = []\n",
    "        current_word_count = 0\n",
    "        chunk_index = 0\n",
    "\n",
    "        for doc in docs_in_group:\n",
    "            words = doc.page_content.split()\n",
    "            n_words = len(words)\n",
    "\n",
    "            # If adding this utterance would exceed the max_words, flush current chunk\n",
    "            if current_docs and (current_word_count + n_words > self.max_words):\n",
    "                chunk_doc = self._make_chunk_document(\n",
    "                    episode, act, chunk_index, current_docs\n",
    "                )\n",
    "                chunks.append(chunk_doc)\n",
    "                chunk_index += 1\n",
    "\n",
    "                # prepare next chunk, with overlap\n",
    "                if self.overlap_utterances > 0:\n",
    "                    overlap_docs = current_docs[-self.overlap_utterances:]\n",
    "                else:\n",
    "                    overlap_docs = []\n",
    "\n",
    "                current_docs = list(overlap_docs)\n",
    "                current_word_count = sum(\n",
    "                    len(d.page_content.split()) for d in current_docs\n",
    "                )\n",
    "\n",
    "            # add current utterance\n",
    "            current_docs.append(doc)\n",
    "            current_word_count += n_words\n",
    "\n",
    "        # flush final chunk\n",
    "        if current_docs:\n",
    "            chunk_doc = self._make_chunk_document(\n",
    "                episode, act, chunk_index, current_docs\n",
    "            )\n",
    "            chunks.append(chunk_doc)\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def chunk_documents(self, docs: List[Document]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        High-level method: groups by (episode, act), sorts within each group,\n",
    "        and returns a flat list of chunk Documents.\n",
    "        \"\"\"\n",
    "        grouped = group_by_episode_act(docs)\n",
    "        all_chunks: List[Document] = []\n",
    "\n",
    "        for (episode, act), group_docs in grouped.items():\n",
    "            sorted_docs = sort_group_by_utterance_start(group_docs)\n",
    "            group_chunks = self.chunk_group(episode, act, sorted_docs)\n",
    "            all_chunks.extend(group_chunks)\n",
    "\n",
    "        return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6478fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "    metadata[\"episode\"] = record.get(\"episode\")\n",
    "    metadata[\"role\"] = record.get(\"role\")\n",
    "    metadata[\"speaker\"] = record.get(\"speaker\")\n",
    "    metadata[\"act\"] = record.get(\"act\")\n",
    "    metadata[\"utterance_start\"] = record.get(\"utterance_start\")\n",
    "    metadata[\"utterance_end\"] = record.get(\"utterance_end\")\n",
    "    \n",
    "    return metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df03adad",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = JSONLoader(\n",
    "    file_path=\"data/transcripts_full.json\",\n",
    "    jq_schema=\".[].[]\",     \n",
    "    content_key=\"utterance\",  \n",
    "    metadata_func=metadata_func\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fe77ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Joe Franklin?' metadata={'source': '/Users/matthewlu/Downloads/podcast-RAG/data/transcripts_full.json', 'seq_num': 1, 'episode': 'ep-1', 'role': 'interviewer', 'speaker': 'ira glass', 'act': 'prologue', 'utterance_start': 0.17, 'utterance_end': 0.58}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "102cfaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='I'm ready.' metadata={'source': '/Users/matthewlu/Downloads/podcast-RAG/data/transcripts_full.json', 'seq_num': 2, 'episode': 'ep-1', 'role': 'subject', 'speaker': 'joe franklin', 'act': 'prologue', 'utterance_start': 0.58, 'utterance_end': 1.39}\n"
     ]
    }
   ],
   "source": [
    "print(docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ec2f55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original utterances: 163808\n",
      "Chunked docs: 28642\n",
      "{'episode': 'ep-1', 'act': 'prologue', 'chunk_index': 0, 'num_utterances': 15, 'num_words': 342, 'speakers': 'ira glass, joe franklin', 'roles': 'host, interviewer, subject', 'chunk_utterance_start': 0.17, 'chunk_utterance_end': 132.75}\n",
      "Joe Franklin? I'm ready. It's Ira Glass here. Oh, you're the emcee on the show, Ira. I am the emcee on the show. Yes. Oh great. Ira? I-R-A, Ira? Ira, I-R-A. Oh, great. Now hold on one second, Ira. Don't go away. Hello? [UNINTELLIGIBLE]. Call me after 3 o'clock. I have great news for you. Ira. Yes. So listen, Tony. If the phone rings, take it in the back. And then come out and tell me who it is. Ju ...\n"
     ]
    }
   ],
   "source": [
    "#Combine docs into larger chunks\n",
    "chunker = TranscriptChunker(\n",
    "    max_words=350,          # you can tune this\n",
    "    overlap_utterances=2    # you can tune this too\n",
    ")\n",
    "\n",
    "chunked_docs = chunker.chunk_documents(docs)\n",
    "\n",
    "print(f\"Original utterances: {len(docs)}\")\n",
    "print(f\"Chunked docs: {len(chunked_docs)}\")\n",
    "print(chunked_docs[0].metadata)\n",
    "print(chunked_docs[0].page_content[:400], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2bbf43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Nobody hearing my words right now is thinking, \"Oh, man, remember that show, back when it used to be good? That show, I never missed that show back in the old days, back in the first couple years before it got so-called popular. Back when it was still good.\" No, actually, I think that force, that human desire to say that is so strong, to say that \"I was there back when that show was good,\" that force is so strong, it is so basic to who we are as people that I know-- OK, what are we? We are two minutes into the program-- I know that somewhere out there, one or two of you are saying, \"Oh, sure. I used to listen to that show back in the first 30 seconds, back when it used to be really good. Remember back when they used to do all that crazy stuff? When they had that guy on the phone? Remember back then?\" Well, from WBEZ, in the glorious city of Chicago, Illinois. The name of this show is Your Radio Playhouse. I'm your emcee. I'm your emcee, Ira Glass. OK, the idea of this show, this new little show, is stories, some by journalists and documentary producers, like myself, some just regular people telling their own little stories, some by artists, and writers, and performers of all different kinds. And the idea is we're going to bring you stuff you're not going to find anywhere else. And there is also going to be music. And tonight's show, we thought that we would have a theme. Tonight's show is going to be New Beginnings. And to kick things off, I called the man who's had, as best as anybody can tell, the longest running program in the history of television. His name is Joe Franklin, and his program ran for 43 years on local television in New York. And he claims that he invented the talk show format. And I called him to get some advice on how to create a long-running, healthy program.' metadata={'episode': 'ep-1', 'act': 'prologue', 'chunk_index': 1, 'num_utterances': 5, 'num_words': 344, 'speakers': ['ira glass'], 'roles': ['host'], 'chunk_utterance_start': 72.16, 'chunk_utterance_end': 202.1}\n"
     ]
    }
   ],
   "source": [
    "print(chunked_docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43c6f401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28642\n"
     ]
    }
   ],
   "source": [
    "print(len(chunked_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4add6fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing embeddings from memory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nt/6nrb_d696ylb20nhdcw60rfc0000gn/T/ipykernel_10302/433161040.py:34: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_store = Chroma(\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "ValueError: Batch size of 28642 is greater than max batch size of 5461",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 42\u001b[0m\n\u001b[1;32m     34\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m Chroma(\n\u001b[1;32m     35\u001b[0m     embedding_function\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[1;32m     36\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpodcast_transcripts\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Add the pre-computed data directly to the underlying Chroma collection\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# This bypasses LangChain's automatic embedding generation\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# We use upsert to handle cases where documents might already exist\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m vector_store\u001b[38;5;241m.\u001b[39m_collection\u001b[38;5;241m.\u001b[39mupsert(\n\u001b[1;32m     43\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m     44\u001b[0m     embeddings\u001b[38;5;241m=\u001b[39mvectors,\n\u001b[1;32m     45\u001b[0m     metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[1;32m     46\u001b[0m     documents\u001b[38;5;241m=\u001b[39mtexts\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChroma vector store created successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/chromadb/api/models/Collection.py:458\u001b[0m, in \u001b[0;36mCollection.upsert\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update the embeddings, metadatas or documents for provided ids, or create them if they don't exist.\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \n\u001b[1;32m    440\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;124;03m    None\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    449\u001b[0m upsert_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_prepare_upsert_request(\n\u001b[1;32m    450\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m    451\u001b[0m     embeddings\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    455\u001b[0m     uris\u001b[38;5;241m=\u001b[39muris,\n\u001b[1;32m    456\u001b[0m )\n\u001b[0;32m--> 458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_upsert(\n\u001b[1;32m    459\u001b[0m     collection_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m    460\u001b[0m     ids\u001b[38;5;241m=\u001b[39mupsert_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    461\u001b[0m     embeddings\u001b[38;5;241m=\u001b[39mupsert_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    462\u001b[0m     metadatas\u001b[38;5;241m=\u001b[39mupsert_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadatas\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    463\u001b[0m     documents\u001b[38;5;241m=\u001b[39mupsert_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    464\u001b[0m     uris\u001b[38;5;241m=\u001b[39mupsert_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muris\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    465\u001b[0m     tenant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtenant,\n\u001b[1;32m    466\u001b[0m     database\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatabase,\n\u001b[1;32m    467\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/chromadb/api/rust.py:498\u001b[0m, in \u001b[0;36mRustBindingsAPI._upsert\u001b[0;34m(self, collection_id, ids, embeddings, metadatas, documents, uris, tenant, database)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_upsert\u001b[39m(\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    496\u001b[0m     database: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_DATABASE,\n\u001b[1;32m    497\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m--> 498\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbindings\u001b[38;5;241m.\u001b[39mupsert(\n\u001b[1;32m    499\u001b[0m         \u001b[38;5;28mstr\u001b[39m(collection_id),\n\u001b[1;32m    500\u001b[0m         ids,\n\u001b[1;32m    501\u001b[0m         embeddings,\n\u001b[1;32m    502\u001b[0m         metadatas,\n\u001b[1;32m    503\u001b[0m         documents,\n\u001b[1;32m    504\u001b[0m         uris,\n\u001b[1;32m    505\u001b[0m         tenant,\n\u001b[1;32m    506\u001b[0m         database,\n\u001b[1;32m    507\u001b[0m     )\n",
      "\u001b[0;31mInternalError\u001b[0m: ValueError: Batch size of 28642 is greater than max batch size of 5461"
     ]
    }
   ],
   "source": [
    "# 1. Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# 2. Extract texts + metadata\n",
    "texts = [doc.page_content for doc in chunked_docs]\n",
    "metadatas = [doc.metadata for doc in chunked_docs]\n",
    "ids = [str(i) for i in range(len(texts))]\n",
    "\n",
    "# 3. Manual embedding with rate-limit protection\n",
    "batch_size = 20     # safe\n",
    "sleep_time = 0.15   # ~150 ms between batches\n",
    "\n",
    "vectors = []\n",
    "\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch = texts[i:i+batch_size]\n",
    "    vec = embeddings.embed_documents(batch)\n",
    "    vectors.extend(vec)\n",
    "\n",
    "    print(f\"Embedded {i + len(batch)} / {len(texts)}\", end=\"\\r\")\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "print(\"\\nFinished embedding all chunks.\")\n",
    "\n",
    "# 4. Build the Chroma DB\n",
    "vector_store = Chroma.from_embeddings(\n",
    "    embeddings=vectors,\n",
    "    metadatas=metadatas,\n",
    "    ids=ids\n",
    ")\n",
    "\n",
    "print(\"Chroma vector store created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c379853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ChromaDB persistence directory: /Users/matthewlu/Downloads/podcast-RAG/tal_chroma\n",
      "Upserted 30000 / 28642\n",
      "Done writing to Chroma.\n",
      "Chroma vector store created successfully.\n",
      "Upserted 30000 / 28642\n",
      "Done writing to Chroma.\n",
      "Chroma vector store created successfully.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Force garbage collection to release any dangling file locks\n",
    "gc.collect()\n",
    "\n",
    "persist_dir = os.path.join(os.getcwd(), \"tal_chroma\")\n",
    "print(f\"Using ChromaDB persistence directory: {persist_dir}\")\n",
    "\n",
    "try:\n",
    "    client = chromadb.PersistentClient(path=persist_dir)\n",
    "    collection = client.get_or_create_collection(\"tal_collection\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing ChromaDB: {e}\")\n",
    "    print(\"If you see 'attempt to write a readonly database', please RESTART THE KERNEL.\")\n",
    "    raise e\n",
    "\n",
    "max_batch_size = 5000  # below 5461 limit\n",
    "\n",
    "for start in range(0, len(ids), max_batch_size):\n",
    "    end = start + max_batch_size\n",
    "    batch_ids = ids[start:end]\n",
    "    batch_vectors = vectors[start:end]\n",
    "    batch_metas = metadatas[start:end]\n",
    "    batch_docs = texts[start:end]\n",
    "    \n",
    "    collection.upsert(\n",
    "        ids=batch_ids,\n",
    "        embeddings=batch_vectors,\n",
    "        metadatas=batch_metas,\n",
    "        documents=batch_docs,\n",
    "    )\n",
    "    print(f\"Upserted {end} / {len(ids)}\", end=\"\\r\")\n",
    "\n",
    "print(\"\\nDone writing to Chroma.\")\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    client=client,\n",
    "    collection_name=\"tal_collection\",\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "\n",
    "print(\"Chroma vector store created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "46a20a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_words': 219, 'chunk_index': 0, 'chunk_utterance_end': 3540.00052154195, 'num_utterances': 14, 'chunk_utterance_start': 3456.93, 'speakers': 'adam davidson, alex blumberg, announcer, glen pizzolorusso, ira glass', 'act': 'credits', 'episode': 'ep-355', 'roles': 'host, subject'}\n",
      "Alex Blumberg and Adam Davidson. Alex, my voice is so bad, I think maybe you should read the credits. Why don't you do it? All right, Ira. I hope you feel better. I'm going to bring Adam in here to help me to, since I've never done this before. Thanks today to Ellen Weiss at NPR who made this collaboration happen this week between the news division at NPR, where I work, All Things Considered, and This American Life. Where I work. Thanks also to Mary Ann Casavant, Anna Chai, Kevin Byers, the fant ...\n",
      "\n",
      "{'episode': 'ep-73', 'act': 'prologue', 'chunk_utterance_start': 200.18, 'roles': 'host, interviewer, subject', 'speakers': 'ira glass, jim biederman', 'num_utterances': 6, 'num_words': 224, 'chunk_utterance_end': 283.66, 'chunk_index': 3}\n",
      "They do? Yes, I'm off to the side right now, but if I go across-- and I'll just do this for you, Ira, because I know that this is real radio here. So I'm going to go over to the other side here. And I'm in the center now, and she's looking straight at me. She's got that enigmatic smile. She looks like she might be smirking a bit at these tourists. I could be wrong. And now I'm over on the left side, and she's still staring at me over here. Well, today on our program, people who thought that they ...\n",
      "\n",
      "{'chunk_utterance_end': 3464.92, 'chunk_index': 0, 'num_utterances': 1, 'speakers': 'ira glass', 'num_words': 13, 'episode': 'ep-385', 'chunk_utterance_start': 3461.47, 'roles': 'host', 'act': 'credits'}\n",
      "I'm Ira Glass. Back next week with more stories of This American Life. ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = vector_store.similarity_search(\"In episode 462, what did Ira Glass and Steve Blass talk about?\", k=3)\n",
    "for d in docs:\n",
    "    print(d.metadata)\n",
    "    print(d.page_content[:500], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f915168e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens across 28642 chunks: 10993831\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "\n",
    "# chunked_docs is a list of Document objects, but encode expects a string.\n",
    "# We need to iterate and encode the page_content of each doc.\n",
    "total_tokens = 0\n",
    "for doc in chunked_docs:\n",
    "    total_tokens += len(tokenizer.encode(doc.page_content))\n",
    "\n",
    "print(f\"Total tokens across {len(chunked_docs)} chunks: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9d7eed03",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=1,\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    verbose=False,\n",
    "    streaming=False\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ca53d5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def ask_podcast_rag(question: str):\n",
    "    # 1. Try to extract an episode number for metadata filtering\n",
    "    # This helps the vector store narrow down to the specific episode\n",
    "    search_kwargs = {\"k\": 10}\n",
    "    \n",
    "    # Regex to find \"episode <number>\"\n",
    "    match = re.search(r\"episode\\s+(\\d+)\", question, re.IGNORECASE)\n",
    "    filter_used = None\n",
    "    if match:\n",
    "        ep_num = match.group(1)\n",
    "        # Construct the ID format used in your metadata (e.g., \"ep-462\")\n",
    "        filter_dict = {\"episode\": f\"ep-{ep_num}\"}\n",
    "        search_kwargs[\"filter\"] = filter_dict\n",
    "        filter_used = filter_dict\n",
    "    \n",
    "    # 2. Retrieve documents\n",
    "    docs = vector_store.similarity_search(question, **search_kwargs)\n",
    "    \n",
    "    # 3. Format context\n",
    "    context = \"\"\n",
    "    for i, doc in enumerate(docs):\n",
    "        context += f\"\\nDocument {i+1} (Episode {doc.metadata.get('episode')}):\\n{doc.page_content}\\n\"\n",
    "    \n",
    "    # 4. Build prompt\n",
    "    prompt = f\"\"\"You are a helpful assistant answering questions about podcast transcripts.\n",
    "Use the following context, and also general knowledge to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "    # 5. Get answer\n",
    "    response = llm.predict(prompt)\n",
    "    \n",
    "    return response, docs, filter_used\n",
    "\n",
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "edc97e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying metadata filter: {'episode': 'ep-500'}\n",
      "Answer:\n",
      "Ira Glass compares the milestone of the 500th episode to an odometer clicking over.\n",
      "\n",
      "--------------------------------------------------\n",
      "Context Used:\n",
      "Document 1 (Episode ep-500):\n",
      "From WBEZ Chicago, it's This American Life, distributed by Public Radio International. I'm Ira Glass, and this is our 500th episode. And what does that feel like? Well, it feels like both a milestone and it feels like nothing. It fills like an odometer clicking over. I was talking to the show's senior producer, Julie Snyder, about this. She's been here for 15 of the show's 17 years, since episode number 58. Five hun-- what-- I just-- I just come to work. I do my job. I go home. Play each ballgame the best I can. Yeah, exactly. No, it really is a blur. It is a blur. And over the last few weeks here at the radio show, we talked about what we should do for the 500th episode. And like, first of all, should we mark it at all? You know what I mean? Like 500 shows on the radio actually isn't that big of a deal for most programs. Like Terry Gross, she knocks through 500 shows like every two years. Doesn't even notice. Just spins right by them. Lots of shows are like that. But, you know, the kind of show that we do here, it takes three or four months to make each episode. So that is different. And besides that, you know, 500, the number 500-- that is a number you notice. And if we didn't do something today, like that would feel weird, you know? So anyway, so as we talked about different things that we could do for this episode, at some point, Julie had the idea that it might be fun to go back to the archives and choose favorite moments from the last 499 episodes. And all this hour, you're going to be hearing the producers of the program, the people who find the stories and who put them together each week, talking about their very favorite moments that we have ever put on the radio.\n",
      "\n",
      "Document 2 (Episode ep-500):\n",
      "It's This American Life. I'm Ira Glass. It is our 500th episode. We're spending it by having the producers of the program come into the studio and pick their favorite moments from the last 17 and 1/2 years of shows. God, it sounds like so many. Well, four of the producers of the show, when they came in to talk about the favorites, they said they thought of all the possible stories they could play today, the story that meant the most to them was the one when they first heard our show or first noticed it doing something different from other radio shows. For me, it was sort of like, what-- what is this thing? That's Jonathan Goldstein. He was a producer on the show for a few years, and he's continued to do stories for us since. Longtime listeners may remember the story of the phone message, \"You and the Little Mermaid can go eff yourselves.\" Anyway, he's now the host of the Public Radio show and podcast called WireTap. And he said this about his first time hearing our show. There was a particular kind of mood to it, and it just-- it really, it sucked me in. You know which one I'm talking about? No, I don't. It's the top, the prologue, to \"The Cruelty of Children.\" Oh, that's so interesting. No one has mentioned that one at all, and that's totally a favorite of mine. Really? Well, OK, so I haven't heard it in years. And so I was surprised by a couple things. One, that it was two minutes long. Wow, is that true? It's that fast? Yeah. And it's so funny, because in my memory, it was like half the show or something, which is insane. And you come on and you're saying, I was talking to a first-grader about libraries-- I was talking to a first-grader about libraries. Then I suddenly found myself talking about bullies.\n",
      "\n",
      "Document 3 (Episode ep-500):\n",
      "Well, our program was produced today by Sarah Koenig and myself, with Alex Blumberg, Ben Calhoun, Miki Meek, Jonathan Menjivar, Lisa Pollak, Brian Reed, Robyn Semien, Alissa Shipp, and Nancy Updike. Our senior producer is Julie Snyder. Thanks today to all the other producers who have worked on the show over the years, who did such a beautiful job making the stories that you heard today-- Peter Clowney, Alix Spiegel, Doris Wilbur, Susan Burton, Blue Chevigny, Wendy Dorr, Starlee Kine, Jonathan Goldstein, Diane Cook, John Jeter, and Jane Feltes AKA Jane MariE. Thanks bigger than I think thanks can actually say to Torey Malatia, who I started the radio show with at WBEZ. Production help today from Thea Benin. Seth Lind is our operations director. Emily Condon's our production manager. Elise Bergerson's our administrative assistant. Music help from Damien Graef and Robb Geddes. Our website, where we have added all kinds of extra stuff this week for the 500th episode-- thanks to Rich Orris for helping with that. And where you can listen to all 500 episodes for absolutely free on your computer or using our app on your cellphone or iPad. We also have lists that we hope are helpful of listeners' favorite shows at our website, thisamericanlife.org. This American Life is distributed by Public Radio International. I hope my father heard me read these credits and felt I did OK. WBEZ management oversight for our show by our boss, Mr. Torey Malatia, who has this message today. Hello, I'm Torey Malatia. I am a real person. Happy 500th episode. Now get back to work. I'm Ira Glass. Back next week with more stories of This American Life. PRI. Public Radio International.\n",
      "\n",
      "Document 4 (Episode ep-500):\n",
      "But, you know, the kind of show that we do here, it takes three or four months to make each episode. So that is different. And besides that, you know, 500, the number 500-- that is a number you notice. And if we didn't do something today, like that would feel weird, you know? So anyway, so as we talked about different things that we could do for this episode, at some point, Julie had the idea that it might be fun to go back to the archives and choose favorite moments from the last 499 episodes. And all this hour, you're going to be hearing the producers of the program, the people who find the stories and who put them together each week, talking about their very favorite moments that we have ever put on the radio. And the reasons that the producers picked their favorites, I think, are different than the reasons that most listeners would give. Some of them chose stories that had been, I have to say, completely forgotten by the rest of the staff for years. Some chose just, like, one little section of script. Some chose, like, a scene that secretly made them cry and they never told the rest of us. They said things like this. When I heard that, I was like, wow, that's one articulate bank robber. And then as it unfolds, you just realize, like, no, no, no, no, no, no, no. I felt like I'd been hit by a car or something. But in a nice way. [LAUGHS] It just made me really mad. Do you remember this? (STAMMERING) It's mind-blowing. And so I am really excited to play you the stuff that we have found. And let's just jump in.\n",
      "\n",
      "Document 5 (Episode ep-500):\n",
      "Yeah. God, I totally did not remember saying that until just now. Yeah, that's so true of me. It's so personal. And I feel like it's really-- I don't know. Like I've known you for 10 years now, right? And I heard you say that. I was like, oh, right. That's right. Because I was like, I know there's a lot of times in interviews where I've just been listening in, and you'll reveal this thing, and I'm always just like, that's ballsy. To me that's just so obvious that you would do that if you have something like that to do, because it's good tape. Like your job is to make good tape. You know what I mean? Like that's our job, is to make good tape. I know, but I feel like that's the thing that's different, right? Like you're willing to kind of exploit anything you've got in there. And I think a lot of people, for a lot of people, that stuff is just off-limits. Sarah Koenig. Coming up, me and Bruce Banner, so similar. And more favorites from the past 499 episodes. That's in a minute. From Chicago Public Radio and Public Radio International, saying this now for the 500th time, when our program continues.\n",
      "\n",
      "Document 6 (Episode ep-500):\n",
      "Alex Blumberg. So as we've approached the 500th episode, I've been asked a lot about what stories and moments over the years are my favorites. And I'm asked that a lot in general, but that has totally stepped up lately. And some of my favorites really are the same shows, I think, that listeners pick when they pick their favorites. There's the camp show. The aircraft carrier. \"Harper High School.\" \"Switched at Birth.\" Sarah Koenig's episode about the two Dr. Gilmers, recently. Starlee Kine's break-up story. \"The Giant Pool of Money.\" The Harold Washington show-- forgive me for half minute, those of you who don't know our show so well, that I'm naming stuff without playing clips. There's no time to play all this stuff. But I think honestly-- and I'm not totally proud of this-- a lot of my favorite moments are just things that I like for completely selfish reasons. Like I got a chance to say something that meant a lot to me for whatever reason. Or really, when the experience of making a story, just the experience of making it, meant a lot to me, like working on the episode last year after David Rakoff died. Or back in the year 2000, I visited David Sedaris in Paris for a show. Or making a batch of Coca-Cola from what we believe is one of the original recipes. Or years ago, going to Medieval Times with a medieval scholar, who's now passed away. Or going to Georgia over and over and over and getting to know people and trying to get them to speak on the record about this small-town judge for the story I was doing. Or my parents used to be on the show a lot, a lot, in the early years of the show. And I'll play you a clip. This is my mom, from our very first episode. Just listen to how skeptical she sounds at first in this clip. Hi. Hi, Mom? Yeah. Can I record a quick conversation with you about something? What about?\n",
      "\n",
      "Document 7 (Episode ep-500):\n",
      "And I'm not-- I wasn't saying that to make you feel guilty about not calling me. I was just-- (TEARFULLY) No, it's hard to explain. It's just, like, you know, you just don't want to intrude into your and Ethan's life and say, one more time, one more time, one more time. Well, I wouldn't have minded. Well, I didn't know about it. I would rather have done that than wind up down here for four months. Yeah, I know that. It's part of the whole cycle, is that you don't-- you don't want to tell anybody else, because when you tell somebody else, then you have to-- you're telling yourself. Which is the last thing you want to do. Isn't that good? Yeah. OK, so what else you got? So there's one-- I don't know if it's too weirdly self-referential or sycophantic or something. But there's this really nice moment of you and Tami Sagher that I came across. It's not one that's in my pantheon at all, but just as I was looking back at old stories, I was like, oh, what's this one? And I listened back. And I realized some of the things I like best on the show are when you're just interviewing someone, and you get a little off-topic, and suddenly something really revealing happens. I feel like it happens kind of a lot when you're interviewing people. And I don't know. You say stuff in interviews that I find really surprising. And so anyway, there's this nice moment in-- I think it's show 314. It's a show called \"It's Never Over.\" And it's the last act. And it's just this interview with Tami Sagher about this joke that she'd thought up like four years earlier and never deployed, because it was a topical joke about Ralph Nader. And then this opportunity comes for her to deploy this joke. Tami Sagher, I should say, is a comedy writer, and she was deploying this joke in a room full of writers at her brand new job.\n",
      "\n",
      "Document 8 (Episode ep-500):\n",
      "And so anyway, there's this nice moment in-- I think it's show 314. It's a show called \"It's Never Over.\" And it's the last act. And it's just this interview with Tami Sagher about this joke that she'd thought up like four years earlier and never deployed, because it was a topical joke about Ralph Nader. And then this opportunity comes for her to deploy this joke. Tami Sagher, I should say, is a comedy writer, and she was deploying this joke in a room full of writers at her brand new job. And she just nails it, and everyone thinks it's this off-the-cuff hilarious comment, not knowing that she'd been saving it up for all this time. And it, like, gets teed up for her perfectly, and she just, like, lets it off really casually, and everybody laughs. And there's this moment right after that where the anecdote has been told. She has the slightly larger thought, like you'd think, done. And then it keeps going, and it's good. And that's at-- I don't know if you want to play it, but that's at 50:22 of that show. Hold on. And so are you done? Are you in? Is that it? Like you used that joke to kind of get you over the hump, and now is it through? Um, I felt like I'm more in, yeah. I don't know that I'll ever feel like I'm in with everybody. [LAUGHS] But no, I have felt like I'm more in. I have felt a marked difference. I mean, do you ever feel like you're totally in with anybody? Dude, I'm married to somebody who I feel like I'm constantly in a situation of having-- I feel like-- and she doesn't feel this way, but I totally feel like every day I have to prove myself anew.\n",
      "\n",
      "Document 9 (Episode ep-500):\n",
      "Can I record a quick conversation with you about something? What about? Well, you know the new show goes on the air this week. Yeah. Are you and dad still worried about me making a living in public radio? I mean, I know just for years, you were urging me just to get out and get basically any job in TV that I possibly could. But now that I've got my own show, are you guys still worried? Or do you feel like things are going OK? Now that Hugh Grant is such a big star, and everybody who sees you or sees your picture thinks how much you look like Hugh Grant, that sort of fires up that TV thing again in me. [LAUGHS] I'm just going to stop that clip. Can I just say? I look nothing like Hugh Grant. Only my mother could think I look like Hugh Grant. Anyway, one of the big surprises, actually, of doing the radio show is that it brought me a lot closer with my parents. Things were pretty chilly between us before then because they disapproved of what I was doing with my life. I have joked in the past that my parents are the only Jews in America who do not like Public Radio. But having them be part of this thing that meant so much to me, being on the radio show, really made us closer in a way that I never anticipated could have happened. And they came around on the Public Radio stuff. Here's my dad from show number 94-- this is February 1998-- coming on the air with some constructive criticism about the way that I read the show's credits. Sometimes you just, you know, roll right through them without a lot of emotion, or maybe without sounding like a lot of caring. And well, you're just not giving it enough importance. You're anxious to beat the clock or something like that, or to get to a cup of coffee, or I don't know what. Yeah. So give me some pointers. Give me some how-to.\n",
      "\n",
      "Document 10 (Episode ep-500):\n",
      "OK, so I'd like to end the show today with this last story. This is one of Julie's picks. Here she is. And the only reason I bring up this one is because I know a lot of people have told me, oh, that they've listened to the show and cried, or they hear a lot of different stories and it makes them cry. Or I don't like listening to the show sometimes, because it totally makes me cry. I'm a little more hard-hearted. I don't cry as easily. I'm always like a little dumbfounded when people tell me about the things that made them cry. Sure, it's sad. I'm not gonna cry. Also, there's just the working here. Once you've heard something a couple of times and stuff, it doesn't make you cry anymore, you know? There is one story, and I have heard this story so many times. And then I had to listen to just to make sure that it held up before I came and talked to you. I cried. Seriously, like throat catching, cried walking down Seventh Avenue yesterday morning, coming into work. Of where I was like, oh my god, somebody's going to see that I'm crying. To my own show. To my own show. A story that I totally know how this goes. What story is it? I can't believe you don't obviously know. What is it? \"Mama, I'm Sorry.\" Awww. Oh, it gets me every time. OK, all right. So this is from our show \"20 Acts in 60 Minutes.\" Yep. Act 20. \"The Greatest Moment I Ever Saw On a Stage.\" I'll say, first of all, that this moment that I saw caught me completely off-guard. I was at a play where I was not expecting anything special. It was put on by an organization that works with teenagers. Music Theater Workshop is what it's called. And among other things, they get kids who are locked up in Chicago's juvenile detention center, the Audy Home, to write and perform musicals about their lives.\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"In Episode 500, what object does Ira Glass compare the milestone to clicking over?\"\n",
    "answer, source_docs, filter_used = ask_podcast_rag(query)\n",
    "\n",
    "if filter_used:\n",
    "    print(f\"Applying metadata filter: {filter_used}\")\n",
    "\n",
    "# Clean up the response to ensure consistent formatting\n",
    "final_answer = answer.strip()\n",
    "if final_answer.startswith(\"Answer:\"):\n",
    "    final_answer = final_answer[len(\"Answer:\"):].strip()\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(final_answer)\n",
    "print()\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"Context Used:\")\n",
    "for i, doc in enumerate(source_docs):\n",
    "    print(f\"Document {i+1} (Episode {doc.metadata.get('episode')}):\")\n",
    "    print(doc.page_content)\n",
    "    print()\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b16d62ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation on 11 questions...\n",
      "\n",
      "Test 1: What is the title of episode 462 of This American Life?\n",
      "  RAG: âœ…\n",
      "  RAG: âœ…\n",
      "  Vanilla: âŒ\n",
      "\n",
      "Test 2: What is the title of episode 449 of This American Life?\n",
      "  Vanilla: âŒ\n",
      "\n",
      "Test 2: What is the title of episode 449 of This American Life?\n",
      "  RAG: âŒ\n",
      "  RAG: âŒ\n",
      "  Vanilla: âŒ\n",
      "\n",
      "Test 3: Which episode is titled 'In Defense of Ignorance'?\n",
      "  Vanilla: âŒ\n",
      "\n",
      "Test 3: Which episode is titled 'In Defense of Ignorance'?\n",
      "  RAG: âœ…\n",
      "  RAG: âœ…\n",
      "  Vanilla: âŒ\n",
      "\n",
      "Test 4: Which show is hosted by the program described as a weekly public radio program produced by WBEZ Chicago and syndicated by PRX?\n",
      "  Vanilla: âŒ\n",
      "\n",
      "Test 4: Which show is hosted by the program described as a weekly public radio program produced by WBEZ Chicago and syndicated by PRX?\n",
      "  RAG: âœ…\n",
      "  RAG: âœ…\n",
      "  Vanilla: âœ…\n",
      "\n",
      "Test 5: What is the original name of 'This American Life' when it first aired in 1995?\n",
      "  Vanilla: âœ…\n",
      "\n",
      "Test 5: What is the original name of 'This American Life' when it first aired in 1995?\n",
      "  RAG: âŒ\n",
      "  RAG: âŒ\n",
      "  Vanilla: âœ…\n",
      "\n",
      "Test 6: As of 2025, who is listed as the host of This American Life?\n",
      "  Vanilla: âœ…\n",
      "\n",
      "Test 6: As of 2025, who is listed as the host of This American Life?\n",
      "  RAG: âœ…\n",
      "  RAG: âœ…\n",
      "  Vanilla: âœ…\n",
      "\n",
      "Test 7: What is the episode number of 'In Defense of Ignorance'?\n",
      "  Vanilla: âœ…\n",
      "\n",
      "Test 7: What is the episode number of 'In Defense of Ignorance'?\n",
      "  RAG: âœ…\n",
      "  RAG: âœ…\n",
      "  Vanilla: âŒ\n",
      "\n",
      "Test 8: Which This American Life episode archive page indicates that transcripts become available the week after broadcast?\n",
      "  Vanilla: âŒ\n",
      "\n",
      "Test 8: Which This American Life episode archive page indicates that transcripts become available the week after broadcast?\n",
      "  RAG: âŒ\n",
      "  RAG: âŒ\n",
      "  Vanilla: âœ…\n",
      "\n",
      "Test 9: Who is the interviewee in the prologue of Episode 1?\n",
      "  Vanilla: âœ…\n",
      "\n",
      "Test 9: Who is the interviewee in the prologue of Episode 1?\n",
      "  RAG: âœ…\n",
      "  RAG: âœ…\n",
      "  Vanilla: âŒ\n",
      "\n",
      "Test 10: In Episode 200, which government department hired a former ad executive to run an information campaign?\n",
      "  Vanilla: âŒ\n",
      "\n",
      "Test 10: In Episode 200, which government department hired a former ad executive to run an information campaign?\n",
      "  RAG: âœ…\n",
      "  RAG: âœ…\n",
      "  Vanilla: âŒ\n",
      "\n",
      "Test 11: In Episode 500, what object does Ira Glass compare the milestone to clicking over?\n",
      "  Vanilla: âŒ\n",
      "\n",
      "Test 11: In Episode 500, what object does Ira Glass compare the milestone to clicking over?\n",
      "  RAG: âœ…\n",
      "  RAG: âœ…\n",
      "  Vanilla: âœ…\n",
      "--------------------------------------------------\n",
      "Final Results:\n",
      "RAG Accuracy:     72.7%\n",
      "Vanilla Accuracy: 45.5%\n",
      "--------------------------------------------------\n",
      "  Vanilla: âœ…\n",
      "--------------------------------------------------\n",
      "Final Results:\n",
      "RAG Accuracy:     72.7%\n",
      "Vanilla Accuracy: 45.5%\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "golden_set = [\n",
    "    {\n",
    "        \"question\": \"What is the title of episode 462 of This American Life?\",\n",
    "        \"ground_truth\": \"Own Worst Enemy\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the title of episode 449 of This American Life?\",\n",
    "        \"ground_truth\": \"Middle School\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which episode is titled 'In Defense of Ignorance'?\",\n",
    "        \"ground_truth\": \"585\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which show is hosted by the program described as a weekly public radio program produced by WBEZ Chicago and syndicated by PRX?\",\n",
    "        \"ground_truth\": \"This American Life\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the original name of 'This American Life' when it first aired in 1995?\",\n",
    "        \"ground_truth\": \"Your Radio Playhouse\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"As of 2025, who is listed as the host of This American Life?\",\n",
    "        \"ground_truth\": \"Ira Glass\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the episode number of 'In Defense of Ignorance'?\",\n",
    "        \"ground_truth\": \"585\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which This American Life episode archive page indicates that transcripts become available the week after broadcast?\",\n",
    "        \"ground_truth\": \"FAQ page\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Who is the interviewee in the prologue of Episode 1?\",\n",
    "        \"ground_truth\": \"Joe Franklin\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"In Episode 200, which government department hired a former ad executive to run an information campaign?\",\n",
    "        \"ground_truth\": \"US State Department\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"In Episode 500, what object does Ira Glass compare the milestone to clicking over?\",\n",
    "        \"ground_truth\": \"odometer\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def ask_vanilla_llm(question):\n",
    "    \"\"\"Asks the LLM without any retrieved context.\"\"\"\n",
    "    prompt = f\"\"\"You are a helpful assistant. Answer the following question to the best of your ability.\n",
    "    \n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    return llm.predict(prompt)\n",
    "\n",
    "def grade_answer(question, ground_truth, prediction):\n",
    "    \"\"\"\n",
    "    Grades the answer based on string matching rules:\n",
    "    1. Exact match (case-insensitive, stripped)\n",
    "    2. Ground truth is a substring of prediction\n",
    "    3. Prediction is a substring of ground truth\n",
    "    4. All words in ground truth appear in prediction\n",
    "    \"\"\"\n",
    "    gt_norm = ground_truth.lower().strip()\n",
    "    pred_norm = prediction.lower().strip()\n",
    "    \n",
    "    # 1. Exact match\n",
    "    if gt_norm == pred_norm:\n",
    "        return True\n",
    "        \n",
    "    # 2. Ground truth is substring of prediction\n",
    "    if gt_norm in pred_norm:\n",
    "        return True\n",
    "        \n",
    "    # 3. Prediction is substring of ground truth\n",
    "    if pred_norm in gt_norm:\n",
    "        return True\n",
    "        \n",
    "    # 4. All words in ground truth appear in prediction\n",
    "    gt_words = set(gt_norm.split())\n",
    "    pred_words = set(pred_norm.split())\n",
    "    if gt_words.issubset(pred_words):\n",
    "        return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "# 2. Run Evaluation\n",
    "results = []\n",
    "print(f\"Starting evaluation on {len(golden_set)} questions...\")\n",
    "\n",
    "for i, item in enumerate(golden_set):\n",
    "    q = item[\"question\"]\n",
    "    gt = item[\"ground_truth\"]\n",
    "    \n",
    "    print(f\"\\nTest {i+1}: {q}\")\n",
    "    \n",
    "    # Test RAG\n",
    "    # Note: ask_podcast_rag returns (response, docs, filter_used)\n",
    "    rag_ans, _, _ = ask_podcast_rag(q)\n",
    "    rag_correct = grade_answer(q, gt, rag_ans)\n",
    "    print(f\"  RAG: {'âœ…' if rag_correct else 'âŒ'}\")\n",
    "    \n",
    "    # Test Vanilla\n",
    "    vanilla_ans = ask_vanilla_llm(q)\n",
    "    vanilla_correct = grade_answer(q, gt, vanilla_ans)\n",
    "    print(f\"  Vanilla: {'âœ…' if vanilla_correct else 'âŒ'}\")\n",
    "    \n",
    "    results.append({\n",
    "        \"question\": q,\n",
    "        \"ground_truth\": gt,\n",
    "        \"rag_correct\": rag_correct,\n",
    "        \"vanilla_correct\": vanilla_correct,\n",
    "        \"rag_ans\": rag_ans,\n",
    "        \"vanilla_ans\": vanilla_ans\n",
    "    })\n",
    "\n",
    "# 3. Calculate Metrics\n",
    "rag_accuracy = sum(1 for r in results if r[\"rag_correct\"]) / len(results) * 100\n",
    "vanilla_accuracy = sum(1 for r in results if r[\"vanilla_correct\"]) / len(results) * 100\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Final Results:\")\n",
    "print(f\"RAG Accuracy:     {rag_accuracy:.1f}%\")\n",
    "print(f\"Vanilla Accuracy: {vanilla_accuracy:.1f}%\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Optional: Print failures to analyze\n",
    "# print(\"\\nAnalysis of RAG Failures:\")\n",
    "# for r in results:\n",
    "#     if not r[\"rag_correct\"]:\n",
    "#         print(f\"Q: {r['question']}\")\n",
    "#         print(f\"Expected: {r['ground_truth']}\")\n",
    "#         print(f\"Got: {r['rag_ans']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b458fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
